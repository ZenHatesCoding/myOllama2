import base64
import io
import os
import tempfile
import uuid
import asyncio
from PIL import Image
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.documents import Document
from docx import Document as DocxDocument
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_ollama import ChatOllama
from langchain_ollama import OllamaEmbeddings
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from models import state
from mcp import MCPManager, NewsMCP


embedding_model = OllamaEmbeddings(model="nomic-embed-text", base_url="http://localhost:11434")
llm_model = ChatOllama(
    model="qwen3:8b",
    base_url="http://localhost:11434",
    temperature=0.7
)

mcp_manager = MCPManager()
mcp_manager.register_mcp(NewsMCP(api_key="f01f4e17ae8680f4ad7c16904e0a3d21"))


def load_document(file_path, file_type):
    if file_type == "pdf":
        loader = PyPDFLoader(file_path)
        return loader.load()
    elif file_type == "docx":
        doc = DocxDocument(file_path)
        text = "\n".join([para.text for para in doc.paragraphs])
        return [Document(page_content=text, metadata={"source": file_path})]
    elif file_type == "txt":
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        return [Document(page_content=text, metadata={"source": file_path})]
    return []


def process_document(documents):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    texts = text_splitter.split_documents(documents)
    vector_store = FAISS.from_documents(texts, embedding_model)
    return vector_store


def generate_summary(messages):
    try:
        prompt = "è¯·ç”¨ä¸€å¥è¯æ€»ç»“ä»¥ä¸‹å¯¹è¯çš„ä¸»è¦å†…å®¹ï¼š\n\n"
        for msg in messages:
            prompt += f"{msg.role}: {msg.content}\n"
        prompt += "\næ‘˜è¦ï¼š"
        
        response = llm_model.invoke(prompt)
        return response.content.strip()
    except Exception as e:
        print(f"ç”Ÿæˆæ‘˜è¦å¤±è´¥ï¼š{str(e)}")
        return None


def process_image(image_data):
    try:
        if image_data.startswith('data:image'):
            image_data = image_data.split(',')[1]
        
        image_bytes = base64.b64decode(image_data)
        image = Image.open(io.BytesIO(image_bytes))
        
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        max_size = (512, 512)
        image.thumbnail(max_size, Image.LANCZOS)
        
        temp_dir = tempfile.gettempdir()
        temp_path = os.path.join(temp_dir, f"temp_{uuid.uuid4()}.jpg")
        image.save(temp_path, 'JPEG', quality=70)
        
        return temp_path
    except Exception as e:
        print(f"å›¾åƒå¤„ç†å¤±è´¥: {str(e)}")
        return None


def encode_image_to_base64(image_path):
    try:
        with open(image_path, 'rb') as f:
            return base64.b64encode(f.read()).decode('utf-8')
    except Exception as e:
        print(f"å›¾åƒç¼–ç å¤±è´¥: {str(e)}")
        return None


def prepare_messages(conversation, query, system_prompt, images=None):
    messages = [SystemMessage(content=system_prompt)]
    
    total_turns = conversation.get_total_turns()
    
    if total_turns > state.max_context_turns:
        if not conversation.summary:
            early_messages = conversation.messages[:state.max_context_turns * 2]
            summary = generate_summary(early_messages)
            if summary:
                conversation.summary = summary
        
        if conversation.summary:
            messages.append(SystemMessage(content=f"ä¹‹å‰çš„å¯¹è¯æ‘˜è¦ï¼š{conversation.summary}"))
        
        recent_messages = conversation.messages[-state.max_context_turns * 2:]
        for msg in recent_messages:
            if msg.role == "user":
                messages.append(HumanMessage(content=msg.content))
            elif msg.role == "assistant":
                messages.append(AIMessage(content=msg.content))
    else:
        for msg in conversation.messages:
            if msg.role == "user":
                messages.append(HumanMessage(content=msg.content))
            elif msg.role == "assistant":
                messages.append(AIMessage(content=msg.content))
    
    if images and len(images) > 0:
        image_contents = []
        for img in images:
            image_url = f"data:image/jpeg;base64,{img['data']}"
            print(f"å›¾ç‰‡URLé•¿åº¦: {len(image_url)}")
            print(f"å›¾ç‰‡æ•°æ®é•¿åº¦: {len(img['data'])}")
            image_contents.append({
                "type": "image_url",
                "image_url": {
                    "url": image_url
                }
            })
        
        content = [
            {"type": "text", "text": query}
        ] + image_contents
        
        messages.append(HumanMessage(content=content))
    else:
        messages.append(HumanMessage(content=query))
    
    return messages


async def generate_answer(query, model_name=None):
    from langchain_ollama import ChatOllama
    
    try:
        conversation = state.get_current_conversation()
        
        if model_name is None:
            model_name = "qwen3:8b"
        
        print(f"å¼€å§‹ç”Ÿæˆå›žç­”ï¼Œæ¨¡åž‹: {model_name}")
        
        mcp_result = await mcp_manager.detect_intent_and_execute(query)
        if mcp_result and mcp_result.get("success"):
            tool_name = mcp_result.get("tool_name", "")
            formatted_text = mcp_result.get("formatted_text", "")
            print(f"MCPå·¥å…·è°ƒç”¨æˆåŠŸ: {tool_name}")
            
            tool_display_names = {
                "get_headlines": "å¤´æ¡æ–°é—»",
                "get_news_by_type": "åˆ†ç±»æ–°é—»",
                "search_news": "æ–°é—»æœç´¢"
            }
            tool_display_name = tool_display_names.get(tool_name, tool_name)
            
            state.response_queue.put(("chunk", f"ðŸ“° æ­£åœ¨ä»Ž{tool_display_name}èŽ·å–ä¿¡æ¯...\n\n"))
            
            if not state.should_stop:
                full_text = f"ðŸ“° æ­£åœ¨ä»Ž{tool_display_name}èŽ·å–ä¿¡æ¯...\n\n{formatted_text}"
                
                for i in range(0, len(formatted_text), 10):
                    if state.should_stop:
                        break
                    chunk = formatted_text[i:i+10]
                    state.response_queue.put(("chunk", chunk))
                    import time
                    time.sleep(0.01)
                
                if not state.should_stop:
                    conversation.add_message("user", query)
                    conversation.add_message("assistant", full_text)
                    auto_name_conversation(conversation)
                    state.response_queue.put(("done", ""))
                else:
                    state.response_queue.put(("error", "æ“ä½œå·²ä¸­æ–­"))
            else:
                state.response_queue.put(("error", "æ“ä½œå·²ä¸­æ–­"))
            return
        
        llm = ChatOllama(
            model=model_name,
            base_url="http://localhost:11434",
            temperature=0.7
        )
        
        if conversation.vector_store:
            relevant_docs = conversation.vector_store.similarity_search(query, k=4)
            context = "\n\n".join([doc.page_content for doc in relevant_docs]) if relevant_docs else "æ— ç›¸å…³å†…å®¹"
            system_prompt = f"ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚ä»…åŸºäºŽä»¥ä¸‹å†…å®¹å›žç­”é—®é¢˜ï¼š\n\n{context}"
        else:
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¹äºŽåŠ©äººçš„åŠ©æ‰‹"
        
        if state.should_stop:
            state.response_queue.put(("error", "æ“ä½œå·²ä¸­æ–­"))
            return
        
        images = conversation.images
        print(f"å›¾ç‰‡æ•°é‡: {len(images) if images else 0}")
        
        messages = prepare_messages(conversation, query, system_prompt, images)
        print(f"æ¶ˆæ¯æ•°é‡: {len(messages)}")
        
        output_content = ""
        for chunk in llm.stream(messages):
            if state.should_stop:
                output_content += "\n\næ“ä½œå·²ä¸­æ–­"
                state.response_queue.put(("chunk", "\n\næ“ä½œå·²ä¸­æ–­"))
                break
            
            chunk_text = str(chunk.content)
            output_content += chunk_text
            state.response_queue.put(("chunk", chunk_text))

        if not state.should_stop:
            conversation.add_message("user", query)
            conversation.add_message("assistant", output_content)
            auto_name_conversation(conversation)
            state.response_queue.put(("done", output_content))
        else:
            state.response_queue.put(("error", "æ“ä½œå·²ä¸­æ–­"))

    except Exception as e:
        print(f"ç”Ÿæˆå›žç­”å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        state.response_queue.put(("error", f"ç”Ÿæˆå¤±è´¥ï¼š{str(e)}"))
    finally:
        state.is_generating = False
        state.should_stop = False


def auto_name_conversation(conversation):
    if conversation.messages:
        if conversation.name == "æ–°å¯¹è¯":
            for msg in conversation.messages:
                if msg.role == "user":
                    name = msg.content[:20] + ("..." if len(msg.content) > 20 else "")
                    conversation.name = name
                    break
        
        if len(conversation.messages) >= 2:
            summary = generate_summary(conversation.messages)
            if summary:
                conversation.name = summary[:30] + ("..." if len(summary) > 30 else "")
